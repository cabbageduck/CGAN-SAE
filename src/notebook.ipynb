{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de22989b-1cd1-4e16-a460-2a797eab9352",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, f1_score, roc_curve,precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from torch.nn.functional import sigmoid\n",
    "from cgan import Discriminator, Generator \n",
    "import torch.nn.functional as F\n",
    "from axial import  *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a43ac5-d34c-474b-b1f5-cc3cc4cb47e8",
   "metadata": {},
   "source": [
    "Load datsaset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d459fe64-8503-480e-b505-6e494259ad37",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../data/human.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "features = df.iloc[:, 1:]\n",
    "labels = df.iloc[:, 0]\n",
    "feature_names = features.columns.tolist()\n",
    "scaler = StandardScaler()\n",
    "SEED = 42\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "train_indices=[2, 3, 5, 4]\n",
    "test_indices=[0, 1]\n",
    "X_train = features_scaled[train_indices]\n",
    "y_train = labels.iloc[train_indices]\n",
    "X_test = features_scaled[test_indices]\n",
    "y_test = labels.iloc[test_indices]\n",
    "np.random.seed(SEED)\n",
    "n_new_samples = 200\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "train_dataset = TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train.values).long())\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataset = TensorDataset(torch.from_numpy(X_test).float(), torch.from_numpy(y_test.values).long())\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca98a3a-c0b0-443f-a3aa-f2eda54cc4f1",
   "metadata": {},
   "source": [
    "CGAN setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc0cf13-6fbd-4c8f-8aa2-d43607344741",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = {\n",
    "    'latent_dim': 50,\n",
    "    'n_classes': 2,\n",
    "    'img_shape': (1, 30)\n",
    "}\n",
    "generator = Generator(opt['latent_dim'], opt['n_classes'], opt['img_shape']).to(device)\n",
    "discriminator = Discriminator(opt['n_classes'], opt['img_shape']).to(device)\n",
    "adversarial_loss = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "num_epochs =100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b04c70-8ad6-467f-beb1-5bfceb6efc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, (real_imgs, real_labels) in enumerate(train_loader):\n",
    "        batch_size = real_imgs.size(0)\n",
    "        real_imgs = real_imgs.to(device)\n",
    "        real_labels = real_labels.to(device)\n",
    "        valid = torch.ones(batch_size, 1).to(device)\n",
    "        fake = torch.zeros(batch_size, 1).to(device)\n",
    "        z = torch.randn(batch_size, opt['latent_dim']).to(device)\n",
    "        gen_labels = torch.randint(0, opt['n_classes'], (batch_size,)).to(device)\n",
    "        optimizer_G.zero_grad()\n",
    "        gen_imgs = generator(z, gen_labels)\n",
    "        validity = discriminator(gen_imgs, gen_labels)\n",
    "        g_loss = adversarial_loss(validity, valid)\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "        optimizer_D.zero_grad()\n",
    "        real_validity = discriminator(real_imgs, real_labels)\n",
    "        real_loss = adversarial_loss(real_validity, valid)\n",
    "        fake_validity = discriminator(gen_imgs.detach(), gen_labels)\n",
    "        fake_loss = adversarial_loss(fake_validity, fake)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "def generate_samples(generator, n_samples, latent_dim, n_classes, device):\n",
    "    z = torch.randn(n_samples, latent_dim).to(device)\n",
    "    gen_labels = torch.randint(0, n_classes, (n_samples,)).to(device)\n",
    "    with torch.no_grad():\n",
    "        gen_imgs = generator(z, gen_labels)\n",
    "    return gen_imgs.cpu().numpy(), gen_labels.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60994b47-d2ac-4cdb-b2ef-76dbb21f8238",
   "metadata": {},
   "source": [
    "merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d2fb07-d87d-4abd-aaac-0d2a186e59f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_samples, new_sample_labels = generate_samples(generator, n_new_samples, opt['latent_dim'], opt['n_classes'], device)\n",
    "new_samples_reshaped = new_samples.reshape(new_samples.shape[0], -1)\n",
    "n_train_samples = int(n_new_samples * 0.9)\n",
    "n_test_samples = n_new_samples - n_train_samples\n",
    "X_train_augmented = np.vstack([X_train, new_samples_reshaped[:n_train_samples]])\n",
    "y_train_augmented = np.concatenate([y_train, new_sample_labels[:n_train_samples]])\n",
    "X_test_augmented = np.vstack([X_test, new_samples_reshaped[n_train_samples:]])\n",
    "y_test_augmented = np.concatenate([y_test, new_sample_labels[n_train_samples:]])\n",
    "train_dataset_augmented = TensorDataset(torch.from_numpy(X_train_augmented).float(), torch.from_numpy(y_train_augmented).long())\n",
    "train_loader_augmented = DataLoader(train_dataset_augmented, batch_size=30, shuffle=True)\n",
    "test_dataset_augmented = TensorDataset(torch.from_numpy(X_test_augmented).float(), torch.from_numpy(y_test_augmented).long())\n",
    "test_loader_augmented = DataLoader(test_dataset_augmented, batch_size=30, shuffle=False)\n",
    "N=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2fd8a7-9969-4dd4-b414-fd83e5c99e67",
   "metadata": {},
   "source": [
    "SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15da09bb-d2f5-4058-8fdb-3d0bfd8fa2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SparseAutoencoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, sparsity_ratio):\n",
    "        super(SparseAutoencoder, self).__init__()\n",
    "        self.layers1 = nn.Sequential(\n",
    "            nn.Linear(input_size, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, hidden_size),\n",
    "            AxialAttention(in_planes=input_size, out_planes=hidden_size, groups=1)\n",
    "        )\n",
    "        self.drop=nn.Dropout(0.3)\n",
    "        self.linear1=nn.Linear(input_size, 90)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.linear2 = nn.Linear(90, hidden_size)\n",
    "        self.axial = AxialAttention(in_planes=input_size, out_planes=hidden_size, groups=N)\n",
    "        self.encoder = AxialAttention(in_planes=input_size,out_planes=hidden_size,groups=N)\n",
    "        self.decoder = AxialAttention(in_planes=input_size,out_planes=hidden_size,groups=N)\n",
    "        self.sparsity_ratio = sparsity_ratio\n",
    "    def forward(self, x):\n",
    "        x=self.linear1(x)\n",
    "        x = self.drop(x)\n",
    "        x=self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        b, n = x.size()\n",
    "        x = x.view(b, n, 1, 1)\n",
    "        encoded = self.encoder(x)\n",
    "        b, c, h, w = encoded.size()\n",
    "        x = encoded.view(b, c)\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.relu(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        b, n,w,h =decoded.size()\n",
    "        decoded = decoded.view(b, n)\n",
    "        return decoded, encoded\n",
    "    def sparse_loss(self, encoded):\n",
    "        epsilon = 1e-10\n",
    "        sparsity = torch.mean(encoded, dim=0)\n",
    "\n",
    "        sparsity = torch.clamp(sparsity, epsilon, 1 - epsilon)\n",
    "\n",
    "        kl_div = self.sparsity_ratio * torch.log(self.sparsity_ratio / sparsity) + \\\n",
    "                 (1 - self.sparsity_ratio) * torch.log((1 - self.sparsity_ratio) / (1 - sparsity))\n",
    "        return kl_div.sum()\n",
    "input_size = 30\n",
    "hidden_size = 30\n",
    "sparsity_ratio = 0.01\n",
    "sparse_autoencoder = SparseAutoencoder(input_size, hidden_size, sparsity_ratio).to(device)\n",
    "reconstruction_criterion = nn.MSELoss()\n",
    "optimizer_sparse = optim.Adam(sparse_autoencoder.parameters(), lr=0.01)\n",
    "num_epochs_sparse =100\n",
    "batch_size_sparse = 12\n",
    "sparse_loader = DataLoader(TensorDataset(torch.from_numpy(X_train_augmented).float()), batch_size=batch_size_sparse,\n",
    "                           shuffle=True,drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4940eb13-76de-4b9d-80dd-1821534cb889",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs_sparse):\n",
    "    for data in sparse_loader:\n",
    "        inputs = data[0].to(device)\n",
    "        optimizer_sparse.zero_grad()\n",
    "        outputs, encoded = sparse_autoencoder(inputs)\n",
    "        reconstruction_loss = reconstruction_criterion(outputs, inputs)\n",
    "        sparsity_penalty = sparse_autoencoder.sparse_loss(encoded)\n",
    "        loss = reconstruction_loss + sparsity_penalty\n",
    "        loss.backward()\n",
    "        optimizer_sparse.step()\n",
    "    if (epoch%20==0):\n",
    "        print(f'Sparse Autoencoder Epoch [{epoch + 1}/{num_epochs_sparse}], Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73643070-abe9-4b42-947e-45f4da90976f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sparse_autoencoder.eval()\n",
    "\n",
    "\n",
    "all_reconstruction_errors = []\n",
    "all_true_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader_augmented:\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "\n",
    "        reconstructed_data, encoded = sparse_autoencoder(inputs)\n",
    "\n",
    "\n",
    "        reconstruction_error = torch.mean((inputs - reconstructed_data) ** 2, dim=1).cpu().numpy()\n",
    "        all_reconstruction_errors.extend(reconstruction_error)\n",
    "        all_true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "\n",
    "all_reconstruction_errors = np.array(all_reconstruction_errors)\n",
    "all_true_labels = np.array(all_true_labels)\n",
    "\n",
    "original_auc = roc_auc_score(all_true_labels, all_reconstruction_errors)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "78b59634-1fde-4226-88c8-c40cc27d74ed",
   "metadata": {},
   "source": [
    "Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ef7f07-71bb-4454-be8e-bf7cea49e765",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoded_features = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in sparse_loader:\n",
    "        inputs = data[0].to(device)\n",
    "        _, encoded = sparse_autoencoder(inputs)\n",
    "        b,n,h,w=encoded.size()\n",
    "        encoded=encoded.view(b,n)\n",
    "        encoded_features.append(encoded.cpu().numpy())\n",
    "\n",
    "encoded_features = np.concatenate(encoded_features, axis=0)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "pca_features = pca.fit_transform(encoded_features)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(pca_features[:, 0], pca_features[:, 1])\n",
    "plt.title('PCA of Encoded Features')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95c6477-cab4-478e-8974-c90195435da4",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23ab20f-10a5-4998-85b1-b806b8d4b44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def permutation_test(reconstruction_errors, true_labels, n_permutations=1000):\n",
    "    permuted_aucs = []\n",
    "    original_scores = reconstruction_errors\n",
    "\n",
    "    for i in range(n_permutations):\n",
    "\n",
    "        shuffled_labels = shuffle(true_labels, random_state=i)\n",
    "\n",
    "\n",
    "        auc_permuted = roc_auc_score(shuffled_labels, original_scores)\n",
    "        permuted_aucs.append(auc_permuted)\n",
    "\n",
    "    p_value = (np.sum(np.array(permuted_aucs) >= original_auc) + 1.0) / (n_permutations + 1)\n",
    "    return p_value, permuted_aucs\n",
    "\n",
    "p_value, permuted_aucs = permutation_test(all_reconstruction_errors, all_true_labels, n_permutations=1000)\n",
    "print(f\"Permutation Test P-value: {p_value}\")\n",
    "fpr, tpr, thresholds = roc_curve(all_true_labels, all_reconstruction_errors,drop_intermediate=False)\n",
    "roc_auc = roc_auc_score(all_true_labels, all_reconstruction_errors)\n",
    "optimal_idx = np.argmax(tpr - fpr)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "print(f'Optimal Threshold: {optimal_threshold}')\n",
    "predicted_labels = (all_reconstruction_errors > optimal_threshold).astype(int)\n",
    "accuracy = accuracy_score(all_true_labels, predicted_labels)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "auc = roc_auc\n",
    "print(f'AUC: {auc}')\n",
    "recall = recall_score(all_true_labels, predicted_labels)\n",
    "print(f'Recall: {recall}')\n",
    "f1 = f1_score(all_true_labels, predicted_labels)\n",
    "print(f'F1 Score: {f1}')\n",
    "pre=precision_score(all_true_labels, predicted_labels)\n",
    "print(f'precision Score: {pre}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tudypytorch",
   "language": "python",
   "name": "tudypytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
