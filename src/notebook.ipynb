{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de22989b-1cd1-4e16-a460-2a797eab9352",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, f1_score, roc_curve,precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from torch.nn.functional import sigmoid\n",
    "from cgan import Discriminator, Generator \n",
    "import torch.nn.functional as F\n",
    "from axial import  *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a43ac5-d34c-474b-b1f5-cc3cc4cb47e8",
   "metadata": {},
   "source": [
    "Load datsaset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d459fe64-8503-480e-b505-6e494259ad37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:\n",
      "        V2       V3      V4       V5      V6      V7      V8       V9  \\\n",
      "0  10.5386  11.2981  7.1502  11.7298  4.2480  4.2064  4.9099  10.6330   \n",
      "1  10.8804  10.3168  6.5483  10.6240  4.7173  5.7744  5.4343  10.8606   \n",
      "2  11.3799   8.3963  6.4484   9.8227  5.0551  4.5272  5.5301  10.9074   \n",
      "3  11.4261   8.6628  6.4338  10.2619  3.9583  5.2531  5.4067  10.8560   \n",
      "4  11.4389  10.1147  5.8899   9.7978  3.8156  5.8153  4.9225  11.0238   \n",
      "\n",
      "       V10     V11  ...  V15252  V15253  V15254  V15255  V15256   V15257  \\\n",
      "0  10.2677  3.9637  ...  4.0661  4.0836  5.5672  8.6559  3.6959  12.5022   \n",
      "1  11.2335  5.7483  ...  3.8044  2.3275  6.1617  8.8044  2.3416  12.6586   \n",
      "2  11.5613  5.0113  ...  3.8785  1.6924  6.1789  8.3119  5.1417  12.6373   \n",
      "3  11.9392  4.9906  ...  4.3889  2.0058  6.0277  8.0372  4.3243  12.7094   \n",
      "4  12.1298  3.7522  ...  4.2283  1.8319  6.3910  8.4345  3.4852  12.9852   \n",
      "\n",
      "   V15258   V15259   V15260  V15261  \n",
      "0  3.1400  11.5148  10.2981  6.9829  \n",
      "1  3.2842  11.4551  10.2020  7.4145  \n",
      "2  3.8568  11.1994  10.2962  7.0779  \n",
      "3  2.4754  11.0417  10.3900  8.1021  \n",
      "4  1.7028  11.2873  10.7049  7.9483  \n",
      "\n",
      "[5 rows x 15260 columns]\n",
      "Labels:\n",
      "0      1\n",
      "1      1\n",
      "2      1\n",
      "3      0\n",
      "4      1\n",
      "      ..\n",
      "577    0\n",
      "578    1\n",
      "579    1\n",
      "580    1\n",
      "581    1\n",
      "Name: V1, Length: 582, dtype: int64\n",
      "Training set:\n",
      "216    1\n",
      "299    1\n",
      "297    1\n",
      "279    1\n",
      "228    0\n",
      "      ..\n",
      "24     1\n",
      "64     1\n",
      "368    1\n",
      "523    1\n",
      "449    1\n",
      "Name: V1, Length: 465, dtype: int64\n",
      "Test set:\n",
      "44     1\n",
      "43     1\n",
      "179    0\n",
      "402    1\n",
      "287    1\n",
      "      ..\n",
      "538    1\n",
      "539    1\n",
      "231    1\n",
      "478    0\n",
      "364    1\n",
      "Name: V1, Length: 117, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "type=0\n",
    "file_path = '../data/human.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "if \"tall.csv\" not in file_path:\n",
    "    type=1\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "features = df.iloc[:, 1:]\n",
    "labels = df.iloc[:, 0]\n",
    "feature_names = features.columns.tolist()\n",
    "if type==1:\n",
    "    features = df.iloc[:, 1:-2]\n",
    "    labels = df.iloc[:, 0]\n",
    "    feature_names = features.columns.tolist()\n",
    "\n",
    "\n",
    "print(\"Features:\")\n",
    "print(features.head())\n",
    "print(\"Labels:\")\n",
    "print(labels)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "\n",
    "train_indices=[2, 3, 5, 4]\n",
    "test_indices=[0, 1]\n",
    "\n",
    "X_train = features_scaled[train_indices]\n",
    "y_train = labels.iloc[train_indices]\n",
    "\n",
    "X_test = features_scaled[test_indices]\n",
    "y_test = labels.iloc[test_indices]\n",
    "\n",
    "if \"tall.csv\" not in file_path:\n",
    "    type=1\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features_scaled, labels, test_size=0.2, random_state=42,\n",
    "                                                        stratify=labels)\n",
    "\n",
    "print(\"Training set:\")\n",
    "print(y_train)\n",
    "print(\"Test set:\")\n",
    "print(y_test)\n",
    "\n",
    "\n",
    "train_dataset = TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train.values).long())\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(torch.from_numpy(X_test).float(), torch.from_numpy(y_test.values).long())\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca98a3a-c0b0-443f-a3aa-f2eda54cc4f1",
   "metadata": {},
   "source": [
    "CGAN setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbc0cf13-6fbd-4c8f-8aa2-d43607344741",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = {\n",
    "    'latent_dim': 50,\n",
    "    'n_classes': 2,\n",
    "    'img_shape': (1, 30)\n",
    "}\n",
    "if type == 1:\n",
    "    opt['img_shape'] = (1, 15260)\n",
    "generator = Generator(opt['latent_dim'], opt['n_classes'], opt['img_shape']).to(device)\n",
    "discriminator = Discriminator(opt['n_classes'], opt['img_shape']).to(device)\n",
    "adversarial_loss = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "num_epochs =100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7b04c70-8ad6-467f-beb1-5bfceb6efc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, (real_imgs, real_labels) in enumerate(train_loader):\n",
    "        batch_size = real_imgs.size(0)\n",
    "        real_imgs = real_imgs.to(device)\n",
    "        real_labels = real_labels.to(device)\n",
    "        valid = torch.ones(batch_size, 1).to(device)\n",
    "        fake = torch.zeros(batch_size, 1).to(device)\n",
    "        z = torch.randn(batch_size, opt['latent_dim']).to(device)\n",
    "        gen_labels = torch.randint(0, opt['n_classes'], (batch_size,)).to(device)\n",
    "        optimizer_G.zero_grad()\n",
    "        gen_imgs = generator(z, gen_labels)\n",
    "        validity = discriminator(gen_imgs, gen_labels)\n",
    "        g_loss = adversarial_loss(validity, valid)\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "        optimizer_D.zero_grad()\n",
    "        real_validity = discriminator(real_imgs, real_labels)\n",
    "        real_loss = adversarial_loss(real_validity, valid)\n",
    "        fake_validity = discriminator(gen_imgs.detach(), gen_labels)\n",
    "        fake_loss = adversarial_loss(fake_validity, fake)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "def generate_samples(generator, n_samples, latent_dim, n_classes, device):\n",
    "    z = torch.randn(n_samples, latent_dim).to(device)\n",
    "    gen_labels = torch.randint(0, n_classes, (n_samples,)).to(device)\n",
    "    with torch.no_grad():\n",
    "        gen_imgs = generator(z, gen_labels)\n",
    "    return gen_imgs.cpu().numpy(), gen_labels.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60994b47-d2ac-4cdb-b2ef-76dbb21f8238",
   "metadata": {},
   "source": [
    "merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1d2fb07-d87d-4abd-aaac-0d2a186e59f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_new_samples = 100\n",
    "new_samples, new_sample_labels = generate_samples(generator, n_new_samples, opt['latent_dim'], opt['n_classes'], device)\n",
    "new_samples_reshaped = new_samples.reshape(new_samples.shape[0], -1)\n",
    "n_train_samples = int(n_new_samples * 0.9)\n",
    "n_test_samples = n_new_samples - n_train_samples\n",
    "X_train_augmented = np.vstack([X_train, new_samples_reshaped[:n_train_samples]])\n",
    "y_train_augmented = np.concatenate([y_train, new_sample_labels[:n_train_samples]])\n",
    "X_test_augmented = np.vstack([X_test, new_samples_reshaped[n_train_samples:]])\n",
    "y_test_augmented = np.concatenate([y_test, new_sample_labels[n_train_samples:]])\n",
    "train_dataset_augmented = TensorDataset(torch.from_numpy(X_train_augmented).float(), torch.from_numpy(y_train_augmented).long())\n",
    "train_loader_augmented = DataLoader(train_dataset_augmented, batch_size=30, shuffle=True)\n",
    "test_dataset_augmented = TensorDataset(torch.from_numpy(X_test_augmented).float(), torch.from_numpy(y_test_augmented).long())\n",
    "test_loader_augmented = DataLoader(test_dataset_augmented, batch_size=30, shuffle=False)\n",
    "N=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2fd8a7-9969-4dd4-b414-fd83e5c99e67",
   "metadata": {},
   "source": [
    "SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15da09bb-d2f5-4058-8fdb-3d0bfd8fa2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SparseAutoencoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, sparsity_ratio):\n",
    "        super(SparseAutoencoder, self).__init__()\n",
    "        self.layers1 = nn.Sequential(\n",
    "            nn.Linear(input_size, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, hidden_size),\n",
    "            AxialAttention(in_planes=input_size, out_planes=hidden_size, groups=1)\n",
    "        )\n",
    "        self.drop=nn.Dropout(0.3)\n",
    "        self.linear1=nn.Linear(input_size, 90)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.linear2 = nn.Linear(90, hidden_size)\n",
    "        self.axial = AxialAttention(in_planes=input_size, out_planes=hidden_size, groups=N)\n",
    "        self.encoder = AxialAttention(in_planes=input_size,out_planes=hidden_size,groups=N)\n",
    "        self.decoder = AxialAttention(in_planes=input_size,out_planes=hidden_size,groups=N)\n",
    "        self.sparsity_ratio = sparsity_ratio\n",
    "    def forward(self, x):\n",
    "        x=self.linear1(x)\n",
    "        x = self.drop(x)\n",
    "        x=self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        b, n = x.size()\n",
    "        x = x.view(b, n, 1, 1)\n",
    "        encoded = self.encoder(x)\n",
    "        b, c, h, w = encoded.size()\n",
    "        x = encoded.view(b, c)\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.relu(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        b, n,w,h =decoded.size()\n",
    "        decoded = decoded.view(b, n)\n",
    "        return decoded, encoded\n",
    "    def sparse_loss(self, encoded):\n",
    "        epsilon = 1e-10\n",
    "        sparsity = torch.mean(encoded, dim=0)\n",
    "\n",
    "        sparsity = torch.clamp(sparsity, epsilon, 1 - epsilon)\n",
    "\n",
    "        kl_div = self.sparsity_ratio * torch.log(self.sparsity_ratio / sparsity) + \\\n",
    "                 (1 - self.sparsity_ratio) * torch.log((1 - self.sparsity_ratio) / (1 - sparsity))\n",
    "        return kl_div.sum()\n",
    "input_size = 30\n",
    "hidden_size = 30\n",
    "if type==1:\n",
    "    input_size = 15260\n",
    "    hidden_size = 15260\n",
    "sparsity_ratio = 0.01\n",
    "sparse_autoencoder = SparseAutoencoder(input_size, hidden_size, sparsity_ratio).to(device)\n",
    "reconstruction_criterion = nn.MSELoss()\n",
    "optimizer_sparse = optim.Adam(sparse_autoencoder.parameters(), lr=0.01)\n",
    "num_epochs_sparse =100\n",
    "batch_size_sparse = 12\n",
    "sparse_loader = DataLoader(TensorDataset(torch.from_numpy(X_train_augmented).float()), batch_size=batch_size_sparse,\n",
    "                           shuffle=True,drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4940eb13-76de-4b9d-80dd-1821534cb889",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.74 GiB. GPU 0 has a total capacty of 4.00 GiB of which 0 bytes is free. Of the allocated memory 9.10 GiB is allocated by PyTorch, and 91.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m     sparsity_penalty \u001b[38;5;241m=\u001b[39m sparse_autoencoder\u001b[38;5;241m.\u001b[39msparse_loss(encoded)\n\u001b[0;32m      8\u001b[0m     loss \u001b[38;5;241m=\u001b[39m reconstruction_loss \u001b[38;5;241m+\u001b[39m sparsity_penalty\n\u001b[1;32m----> 9\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     optimizer_sparse\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (epoch\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m20\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tudypytorch\\lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tudypytorch\\lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.74 GiB. GPU 0 has a total capacty of 4.00 GiB of which 0 bytes is free. Of the allocated memory 9.10 GiB is allocated by PyTorch, and 91.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs_sparse):\n",
    "    for data in sparse_loader:\n",
    "        inputs = data[0].to(device)\n",
    "        optimizer_sparse.zero_grad()\n",
    "        outputs, encoded = sparse_autoencoder(inputs)\n",
    "        reconstruction_loss = reconstruction_criterion(outputs, inputs)\n",
    "        sparsity_penalty = sparse_autoencoder.sparse_loss(encoded)\n",
    "        loss = reconstruction_loss + sparsity_penalty\n",
    "        loss.backward()\n",
    "        optimizer_sparse.step()\n",
    "    if (epoch%20==0):\n",
    "        print(f'Sparse Autoencoder Epoch [{epoch + 1}/{num_epochs_sparse}], Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73643070-abe9-4b42-947e-45f4da90976f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sparse_autoencoder.eval()\n",
    "\n",
    "\n",
    "all_reconstruction_errors = []\n",
    "all_true_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader_augmented:\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "\n",
    "        reconstructed_data, encoded = sparse_autoencoder(inputs)\n",
    "\n",
    "\n",
    "        reconstruction_error = torch.mean((inputs - reconstructed_data) ** 2, dim=1).cpu().numpy()\n",
    "        all_reconstruction_errors.extend(reconstruction_error)\n",
    "        all_true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "\n",
    "all_reconstruction_errors = np.array(all_reconstruction_errors)\n",
    "all_true_labels = np.array(all_true_labels)\n",
    "\n",
    "original_auc = roc_auc_score(all_true_labels, all_reconstruction_errors)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "78b59634-1fde-4226-88c8-c40cc27d74ed",
   "metadata": {},
   "source": [
    "Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ef7f07-71bb-4454-be8e-bf7cea49e765",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoded_features = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in sparse_loader:\n",
    "        inputs = data[0].to(device)\n",
    "        _, encoded = sparse_autoencoder(inputs)\n",
    "        b,n,h,w=encoded.size()\n",
    "        encoded=encoded.view(b,n)\n",
    "        encoded_features.append(encoded.cpu().numpy())\n",
    "\n",
    "encoded_features = np.concatenate(encoded_features, axis=0)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "pca_features = pca.fit_transform(encoded_features)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(pca_features[:, 0], pca_features[:, 1])\n",
    "plt.title('PCA of Encoded Features')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95c6477-cab4-478e-8974-c90195435da4",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23ab20f-10a5-4998-85b1-b806b8d4b44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def permutation_test(reconstruction_errors, true_labels, n_permutations=1000):\n",
    "    permuted_aucs = []\n",
    "    original_scores = reconstruction_errors\n",
    "\n",
    "    for i in range(n_permutations):\n",
    "\n",
    "        shuffled_labels = shuffle(true_labels, random_state=i)\n",
    "\n",
    "\n",
    "        auc_permuted = roc_auc_score(shuffled_labels, original_scores)\n",
    "        permuted_aucs.append(auc_permuted)\n",
    "\n",
    "    p_value = (np.sum(np.array(permuted_aucs) >= original_auc) + 1.0) / (n_permutations + 1)\n",
    "    return p_value, permuted_aucs\n",
    "\n",
    "p_value, permuted_aucs = permutation_test(all_reconstruction_errors, all_true_labels, n_permutations=1000)\n",
    "print(f\"Permutation Test P-value: {p_value}\")\n",
    "fpr, tpr, thresholds = roc_curve(all_true_labels, all_reconstruction_errors,drop_intermediate=False)\n",
    "roc_auc = roc_auc_score(all_true_labels, all_reconstruction_errors)\n",
    "optimal_idx = np.argmax(tpr - fpr)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "print(f'Optimal Threshold: {optimal_threshold}')\n",
    "predicted_labels = (all_reconstruction_errors > optimal_threshold).astype(int)\n",
    "accuracy = accuracy_score(all_true_labels, predicted_labels)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "auc = roc_auc\n",
    "print(f'AUC: {auc}')\n",
    "recall = recall_score(all_true_labels, predicted_labels)\n",
    "print(f'Recall: {recall}')\n",
    "f1 = f1_score(all_true_labels, predicted_labels)\n",
    "print(f'F1 Score: {f1}')\n",
    "pre=precision_score(all_true_labels, predicted_labels)\n",
    "print(f'precision Score: {pre}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tudypytorch",
   "language": "python",
   "name": "tudypytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
